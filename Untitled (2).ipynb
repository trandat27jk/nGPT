{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ecdfb77f-1d77-4612-8412-98c4a2da06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from torch.nn.utils.parametrize import register_parametrization\n",
    "\n",
    "\n",
    "class ModelConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    heads: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    bias: bool = False\n",
    "    parametrize: bool = True\n",
    "    factor: int = 4\n",
    "\n",
    "\n",
    "class AttentionConfig:\n",
    "    d: int = -1\n",
    "    groups: int = 1\n",
    "    norm_eps: int = 0\n",
    "    eps: float = 1e-6\n",
    "    init_scale = 1\n",
    "    scale: int = 1\n",
    "\n",
    "\n",
    "class FFConfig:\n",
    "    d: int = -1\n",
    "    groups: int = 1\n",
    "    norm_eps: int = 0\n",
    "    eps: float = 1e-6\n",
    "    init_scale: int = 1\n",
    "    scale: int = 1\n",
    "\n",
    "\n",
    "def exist(v):\n",
    "    return v is not None\n",
    "\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exist(v) else d\n",
    "\n",
    "\n",
    "def l2Norm(x, d=-1, groups=1, eps=1e-6, norm_eps=0):\n",
    "    eps = default(eps, 1e-5 if x.dtype == torch.float16 else 1e-10)\n",
    "\n",
    "    if groups > 1:\n",
    "        x = x.chunk(groups, dim=d)\n",
    "        x = torch.stack(x)\n",
    "\n",
    "    if norm_eps == 0:\n",
    "        x_norm = F.normalize(x, dim=d, p=2, eps=eps)\n",
    "\n",
    "    if norm_eps != 0:\n",
    "        norm = x.norm(dim=d, keepdim=True)\n",
    "        d_norm = norm.detach().clamp(min=1 - norm_eps, max=1 + norm_eps)\n",
    "        divisor = norm / d_norm\n",
    "        x_norm = x / divisor.clamp(min=eps)\n",
    "\n",
    "    if groups > 1:\n",
    "        x_norm = torch.cat([*x_norm], dim=d)\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, d=-1, groups=1, eps=1e-6, norm_eps=0):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.groups = groups\n",
    "        self.eps = eps\n",
    "        self.norm_eps = norm_eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return l2Norm(\n",
    "            x, d=self.d, groups=self.groups, eps=self.eps, norm_eps=self.norm_eps\n",
    "        )\n",
    "\n",
    "\n",
    "class LinearNormWeight(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        parametrize=False,\n",
    "        groups=1,\n",
    "        d=-1,\n",
    "        eps=1e-6,\n",
    "        norm_eps=0,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = groups**-1\n",
    "        self.parametrize = parametrize\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "        self.L2Norm = L2Norm(d, groups, eps, norm_eps)\n",
    "        if parametrize:\n",
    "            register_parametrization(self.linear, \"weight\", self.L2Norm)\n",
    "\n",
    "        self.norm_weight_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def norm_weight_(self):\n",
    "        if self.parametrize:\n",
    "            norm = self.weights\n",
    "            original = self.linear.parametrizations.weight.original\n",
    "            original.copy_(norm)\n",
    "        else:\n",
    "            self.weights.copy_(self.L2Norm(self.weights))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.linear.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * self.scale\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, dim, init_scale=1, scale=1):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.ones(dim) * scale)\n",
    "        self.divide_scale = init_scale / scale\n",
    "\n",
    "    def forward(self):\n",
    "        return self.params * self.divide_scale\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, args_attn: AttentionConfig):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.to_q = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "        self.to_k = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "        self.to_v = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "\n",
    "        self.dim_head = args.n_embd // args.heads\n",
    "        self.n_heads = args.heads\n",
    "        self.softmax_scale = self.dim_head**0.5\n",
    "        self.q_scale = Scale(args.n_embd, 1, args.n_embd ** (-0.5))\n",
    "        self.k_scale = Scale(args.n_embd, 1, args.n_embd ** (-0.5))\n",
    "        self.rotary_embed=RotaryEmbedding(self.dim_head)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(\n",
    "                torch.ones(args.block_size, args.block_size).view(\n",
    "                    1, 1, args.block_size, args.block_size\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        self.c_proj = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\n",
    "        q = self.rotary_embed.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_embed.rotate_queries_or_keys(k)\n",
    "\n",
    "        q = q * rearrange(self.q_scale(), \"(h d) -> h 1 d\", h=self.n_heads)\n",
    "        k = k * rearrange(self.q_scale(), \"(h d) -> h 1 d\", h=self.n_heads)\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2)\n",
    "\n",
    "        attn = attn * self.softmax_scale\n",
    "\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = torch.matmul(attn, v)\n",
    "        out = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.c_proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, args_ffn: FFConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = args.factor * args.n_embd\n",
    "        self.w1 = LinearNormWeight(args.n_embd, hidden_dim)\n",
    "        self.w2 = LinearNormWeight(hidden_dim, args.n_embd)\n",
    "        self.w3 = LinearNormWeight(args.n_embd, hidden_dim)\n",
    "\n",
    "        self.scale_u = Scale(\n",
    "            hidden_dim, init_scale=args_ffn.init_scale, scale=args_ffn.scale\n",
    "        )\n",
    "        self.scale_v = Scale(\n",
    "            hidden_dim, init_scale=args_ffn.init_scale, scale=args_ffn.scale\n",
    "        )\n",
    "        self.scale_ = hidden_dim**0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = self.w1(x)*self.scale_u()\n",
    "        \n",
    "        v = self.w3(x)*self.scale_v()\n",
    "\n",
    "        v = v * self.scale_\n",
    "\n",
    "        return self.w2(F.silu(v) * u)\n",
    "\n",
    "\n",
    "class Lerp_Residual(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, index_layer, fc):\n",
    "        super().__init__()\n",
    "        self.fc = fc\n",
    "        self.l2Norm = L2Norm(d=-1)\n",
    "        self.scale = Scale(\n",
    "            args.n_embd, init_scale=(0.05 / (index_layer+1)), scale=args.n_embd ** (-0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        connect_ = x\n",
    "        out = self.l2Norm(self.fc(x, **kwargs))\n",
    "        out = torch.lerp(connect_, out, self.scale())\n",
    "\n",
    "        return self.l2Norm(out)\n",
    "\n",
    "\n",
    "class nGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, args: ModelConfig, args_attn: AttentionConfig, args_ffn: FFConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_layer = args.n_layer\n",
    "        self.n_attn_layeers = nn.ModuleList(\n",
    "            [Attention(args, args_attn) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.n_ffn_layers = nn.ModuleList(\n",
    "            [FeedForward(args, args_ffn) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.residual_attn = nn.ModuleList(\n",
    "            [\n",
    "                Lerp_Residual(args, i, self.n_attn_layeers[i])\n",
    "                for i in range(args.n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_ffn = nn.ModuleList(\n",
    "            [Lerp_Residual(args, i, self.n_ffn_layers[i]) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.to_logits = nn.Linear(args.n_embd, args.vocab_size)\n",
    "        self.to_embedding=nn.Embedding(args.vocab_size,args.n_embd)\n",
    "        \n",
    "    def forward(self, x,targets=None):\n",
    "        \n",
    "        x=self.to_embedding(x)\n",
    "        B, T, C = x.size()\n",
    "        for residual_attn, residual_ffn in zip(self.residual_attn, self.residual_ffn):\n",
    "            x = residual_attn(x)\n",
    "            x = residual_ffn(x)\n",
    "        logits = self.to_logits(x)\n",
    "        if targets is not None:\n",
    "            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index=-1)\n",
    "        else: \n",
    "            loss=None\n",
    "\n",
    "        return loss,logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b66ac676-1436-41d2-b585-b9fcbcfbc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2f8e5e3-9942-4cff-a30b-5c20be49d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "41b1d175-5b5c-492a-961d-b7999bd659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nGPT(ModelConfig,AttentionConfig,FFConfig).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e6ddc75b-c9b7-4e68-bd7b-11591e07cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 190674432\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fde67717-b9ea-4b0e-a83e-c04960be1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa3202aa-a2f0-4a49-9616-12ed7d685c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f14d224-363b-42bb-9fec-3f017f23de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c94a9b38-ed60-4166-b254-8269f6c0e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "073b976c-e3a5-4ff7-a702-e7a02002f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nGPTDataset(Dataset):\n",
    "    def __init__(self,txt,tokenizer,block_size,stride):\n",
    "        super().__init__()\n",
    "        self.input_ds=[]\n",
    "        self.target_ds=[]\n",
    "        tokens_data=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0,len(tokens_data)-block_size,stride):\n",
    "            inputs=tokens_data[i:i+block_size]\n",
    "            targets=tokens_data[i+1:i+block_size+1]\n",
    "            self.input_ds.append(inputs)\n",
    "            self.target_ds.append(targets)\n",
    "    def __len__(self):\n",
    "        return len(self.input_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ds[idx],self.target_ds[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt,block_size=256,stride=128,batch_size=4,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=nGPTDataset(txt,tokenizer,block_size,stride)\n",
    "    print(len(dataset.input_ds[1]))\n",
    "    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers,collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddf6ed-e6ae-4d99-b086-0b51b1c619ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9e1e8678-b8f2-4f62-b243-d6a0b0adb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\",\"r\") as f:\n",
    "    data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "27adb22f-73ba-4300-9d81-e88b7741754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "dataloader=create_dataloader(data,block_size=256,stride=256,batch_size=4,shuffle=True,drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "631e77b0-de2d-4e07-b436-b2b69d5d3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34126f69-4879-4364-913e-08e16b8d3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.824902534484863\n",
      "10.809331893920898\n",
      "10.796067237854004\n",
      "10.78587532043457\n",
      "10.776914596557617\n",
      "10.773719787597656\n",
      "10.751914978027344\n",
      "10.746347427368164\n",
      "10.730432510375977\n",
      "10.7125825881958\n",
      "10.698067665100098\n",
      "10.683918952941895\n",
      "10.664010047912598\n",
      "10.640602111816406\n",
      "10.6179780960083\n",
      "10.607752799987793\n",
      "10.59593391418457\n",
      "10.55465316772461\n",
      "10.545076370239258\n",
      "10.524458885192871\n",
      "10.51523208618164\n",
      "10.48146915435791\n",
      "10.473435401916504\n",
      "10.450030326843262\n",
      "10.405782699584961\n",
      "10.379457473754883\n",
      "10.366362571716309\n",
      "10.310890197753906\n",
      "10.31900405883789\n",
      "10.259428024291992\n",
      "10.24765396118164\n",
      "10.176809310913086\n",
      "10.195982933044434\n",
      "10.157027244567871\n",
      "10.110694885253906\n",
      "10.087054252624512\n",
      "10.040319442749023\n",
      "10.02815055847168\n",
      "9.956095695495605\n",
      "9.893836975097656\n",
      "9.907533645629883\n",
      "9.919057846069336\n",
      "9.869524002075195\n",
      "9.774945259094238\n",
      "9.761590957641602\n",
      "9.675058364868164\n",
      "9.745437622070312\n",
      "9.6355619430542\n",
      "9.601641654968262\n",
      "9.550468444824219\n",
      "9.571961402893066\n",
      "9.493193626403809\n",
      "9.424906730651855\n",
      "9.385324478149414\n",
      "9.388123512268066\n",
      "9.309271812438965\n",
      "9.258345603942871\n",
      "9.193145751953125\n",
      "9.179938316345215\n",
      "9.092574119567871\n",
      "9.048436164855957\n",
      "9.061614036560059\n",
      "9.062268257141113\n",
      "8.94812297821045\n",
      "8.877820014953613\n",
      "8.906641006469727\n",
      "8.853805541992188\n",
      "8.755267143249512\n",
      "8.746082305908203\n",
      "8.740301132202148\n",
      "8.619929313659668\n",
      "8.624818801879883\n",
      "8.51355266571045\n",
      "8.468391418457031\n",
      "8.579500198364258\n",
      "8.551732063293457\n",
      "8.384115219116211\n",
      "8.261617660522461\n",
      "8.280108451843262\n",
      "8.317551612854004\n",
      "8.180838584899902\n",
      "8.245248794555664\n",
      "8.263592720031738\n",
      "8.241739273071289\n",
      "8.080909729003906\n",
      "8.211103439331055\n",
      "7.906489849090576\n",
      "7.983801364898682\n",
      "7.941619396209717\n",
      "7.957475185394287\n",
      "7.848583698272705\n",
      "7.644948959350586\n",
      "7.919793128967285\n",
      "7.8114824295043945\n",
      "7.742435932159424\n",
      "7.5975117683410645\n",
      "7.602172374725342\n",
      "7.71361780166626\n",
      "7.639621734619141\n",
      "7.650287628173828\n",
      "7.779804706573486\n",
      "7.50004768371582\n",
      "7.539509296417236\n",
      "7.499889850616455\n",
      "7.392332553863525\n",
      "7.517298698425293\n",
      "7.5177154541015625\n",
      "7.286609649658203\n",
      "7.44741678237915\n",
      "7.305244445800781\n",
      "7.2413530349731445\n",
      "7.177121162414551\n",
      "7.284223556518555\n",
      "7.164254665374756\n",
      "7.135683536529541\n",
      "7.266943454742432\n",
      "7.154509544372559\n",
      "7.36864709854126\n",
      "7.126732349395752\n",
      "6.999964714050293\n",
      "7.040851593017578\n",
      "7.035233020782471\n",
      "6.976024627685547\n",
      "7.019040584564209\n",
      "6.908482551574707\n",
      "7.06882381439209\n",
      "6.938409328460693\n",
      "6.799576759338379\n",
      "6.822624206542969\n",
      "6.956462383270264\n",
      "6.944857120513916\n",
      "6.886684894561768\n",
      "6.796695709228516\n",
      "6.9182047843933105\n",
      "6.866728782653809\n",
      "6.734612941741943\n",
      "6.839757442474365\n",
      "6.817759990692139\n",
      "6.790881633758545\n",
      "6.7504191398620605\n",
      "6.676915645599365\n",
      "6.75627326965332\n",
      "6.634832382202148\n",
      "6.956812381744385\n",
      "6.624756336212158\n",
      "6.7256178855896\n",
      "6.58834171295166\n",
      "6.781684398651123\n",
      "6.772966384887695\n",
      "6.612062454223633\n",
      "6.711615085601807\n",
      "6.429215908050537\n",
      "6.5738959312438965\n",
      "6.394893169403076\n",
      "6.486360549926758\n",
      "6.590391635894775\n",
      "6.747856616973877\n",
      "6.520618438720703\n",
      "6.830390930175781\n",
      "6.735513687133789\n",
      "6.44074010848999\n",
      "6.696409225463867\n",
      "6.653412342071533\n",
      "6.656771183013916\n",
      "6.587640285491943\n",
      "6.447517395019531\n",
      "6.4751973152160645\n",
      "6.412639141082764\n",
      "6.342779159545898\n",
      "6.406664848327637\n",
      "6.321748733520508\n",
      "6.327224254608154\n",
      "6.432371139526367\n",
      "6.398531913757324\n",
      "6.430163383483887\n",
      "6.593987941741943\n",
      "6.400338172912598\n",
      "6.388330459594727\n",
      "6.000038146972656\n",
      "6.376918792724609\n",
      "6.45062255859375\n",
      "6.3606462478637695\n",
      "6.378699779510498\n",
      "6.049996376037598\n",
      "6.553815841674805\n",
      "6.312516689300537\n",
      "6.405184268951416\n",
      "6.43414831161499\n",
      "6.448594570159912\n",
      "6.576174259185791\n",
      "6.286941051483154\n",
      "6.3530592918396\n",
      "6.332380771636963\n",
      "6.2840447425842285\n",
      "6.411520481109619\n",
      "6.394077301025391\n",
      "6.538990020751953\n",
      "6.220400810241699\n",
      "6.226131439208984\n",
      "6.238668441772461\n",
      "6.158844470977783\n",
      "6.284435272216797\n",
      "6.22830867767334\n",
      "6.1306986808776855\n",
      "6.093571662902832\n",
      "6.078534126281738\n",
      "6.097403049468994\n",
      "6.014041423797607\n",
      "6.111876964569092\n",
      "6.3023271560668945\n",
      "6.281312465667725\n",
      "6.120309352874756\n",
      "5.815596103668213\n",
      "6.127220630645752\n",
      "6.12291145324707\n",
      "6.115231037139893\n",
      "6.279539108276367\n",
      "6.034827709197998\n",
      "6.155026912689209\n",
      "6.31516695022583\n",
      "6.1486711502075195\n",
      "6.334968090057373\n",
      "6.291222095489502\n",
      "6.214871883392334\n",
      "6.20071268081665\n",
      "6.071993350982666\n",
      "6.401414394378662\n",
      "6.408798694610596\n",
      "5.903338432312012\n",
      "6.1774516105651855\n",
      "6.0888895988464355\n",
      "6.199477195739746\n",
      "6.019049167633057\n",
      "6.2009124755859375\n",
      "6.481424331665039\n",
      "5.949808597564697\n",
      "6.241633415222168\n",
      "6.072880744934082\n",
      "6.2160820960998535\n",
      "6.053614139556885\n",
      "6.184512138366699\n",
      "6.198753356933594\n",
      "6.160884857177734\n",
      "6.207236289978027\n",
      "6.094427108764648\n",
      "5.87137508392334\n",
      "6.306606292724609\n",
      "5.9456377029418945\n",
      "5.9235334396362305\n",
      "6.143653869628906\n",
      "6.138366222381592\n",
      "6.008334636688232\n",
      "6.064985752105713\n",
      "5.946740627288818\n",
      "6.036120414733887\n",
      "5.886112213134766\n",
      "5.890593528747559\n",
      "6.091117858886719\n",
      "5.7758917808532715\n",
      "6.127273082733154\n",
      "6.0388665199279785\n",
      "5.961068153381348\n",
      "5.890800476074219\n",
      "6.134677886962891\n",
      "6.189486503601074\n",
      "5.95293664932251\n",
      "6.024287700653076\n",
      "6.27009391784668\n",
      "6.226972579956055\n",
      "5.966353416442871\n",
      "6.069088935852051\n",
      "5.841397762298584\n",
      "5.875983238220215\n",
      "5.933443546295166\n",
      "5.946544170379639\n",
      "5.8788909912109375\n",
      "5.630419731140137\n",
      "5.53916072845459\n",
      "5.9832377433776855\n",
      "5.846099376678467\n",
      "6.039997577667236\n",
      "5.91928243637085\n",
      "5.883949279785156\n",
      "6.151004791259766\n",
      "5.957152843475342\n",
      "5.764135837554932\n",
      "5.781406879425049\n",
      "5.872525215148926\n",
      "5.962818145751953\n",
      "5.971597194671631\n",
      "6.058879375457764\n",
      "5.702335834503174\n",
      "5.974997520446777\n",
      "5.890170097351074\n",
      "5.748296737670898\n",
      "5.896821975708008\n",
      "5.7548980712890625\n",
      "5.788248538970947\n",
      "5.729503631591797\n",
      "5.8153300285339355\n",
      "5.848523139953613\n",
      "5.7723846435546875\n",
      "6.046710968017578\n",
      "5.904555797576904\n",
      "5.757134437561035\n",
      "5.699667930603027\n",
      "6.0011820793151855\n",
      "5.936029434204102\n",
      "5.870361804962158\n",
      "5.846508502960205\n",
      "5.8056111335754395\n",
      "5.77513313293457\n",
      "5.671867370605469\n",
      "5.71481990814209\n",
      "5.808995246887207\n",
      "5.643213748931885\n",
      "5.747091770172119\n",
      "5.511216163635254\n",
      "5.404488563537598\n",
      "5.8729400634765625\n",
      "5.791083812713623\n",
      "5.925671577453613\n",
      "6.064030647277832\n",
      "5.681629180908203\n",
      "5.919225215911865\n",
      "5.749192237854004\n",
      "5.963216304779053\n",
      "5.648789882659912\n",
      "5.892018795013428\n",
      "5.645149230957031\n",
      "5.677875995635986\n",
      "5.50325870513916\n",
      "5.733545303344727\n",
      "5.647928237915039\n",
      "5.3793416023254395\n",
      "6.002317428588867\n",
      "5.689940452575684\n",
      "5.803953170776367\n",
      "5.581103324890137\n",
      "5.4920268058776855\n",
      "5.689560890197754\n",
      "5.641777992248535\n",
      "5.495689868927002\n",
      "5.884515762329102\n",
      "5.8527350425720215\n",
      "5.853880882263184\n",
      "5.573061466217041\n",
      "5.516696453094482\n",
      "5.739251613616943\n",
      "5.597022533416748\n",
      "5.576418876647949\n",
      "5.710918426513672\n",
      "5.6701202392578125\n",
      "5.5058512687683105\n",
      "5.63455867767334\n",
      "5.6580610275268555\n",
      "5.421870708465576\n",
      "5.59431791305542\n",
      "5.713728427886963\n",
      "5.601341247558594\n",
      "5.7432708740234375\n",
      "5.486669063568115\n",
      "5.514614105224609\n",
      "5.712435722351074\n",
      "5.44639778137207\n",
      "5.549361705780029\n",
      "5.615657329559326\n",
      "5.527463912963867\n",
      "5.821528911590576\n",
      "5.575686454772949\n",
      "5.535641670227051\n",
      "5.549821853637695\n",
      "5.96293830871582\n",
      "5.5109782218933105\n",
      "5.476066589355469\n",
      "5.387049674987793\n",
      "5.517568588256836\n",
      "5.628918170928955\n",
      "5.3245849609375\n",
      "5.693629264831543\n",
      "5.5541486740112305\n",
      "5.5629496574401855\n",
      "5.518777847290039\n",
      "5.433094501495361\n",
      "5.354561805725098\n",
      "5.899752616882324\n",
      "5.449617385864258\n",
      "5.666646957397461\n",
      "5.395362377166748\n",
      "5.4910054206848145\n",
      "5.449234962463379\n",
      "5.540682792663574\n",
      "5.507335186004639\n",
      "5.9277801513671875\n",
      "5.395901679992676\n",
      "5.539823055267334\n",
      "5.267357349395752\n",
      "5.475403785705566\n",
      "5.362131118774414\n",
      "5.655379295349121\n",
      "5.4234490394592285\n",
      "5.1922197341918945\n",
      "5.301961421966553\n",
      "5.537557601928711\n",
      "5.333050727844238\n",
      "5.0837016105651855\n",
      "5.518083572387695\n",
      "5.430545330047607\n",
      "5.337259769439697\n",
      "5.527938365936279\n",
      "5.612948894500732\n",
      "5.473430633544922\n",
      "5.5025954246521\n",
      "5.537959098815918\n",
      "5.814163684844971\n",
      "5.07527494430542\n",
      "5.5306925773620605\n",
      "5.326258659362793\n",
      "5.28364896774292\n",
      "5.320307731628418\n",
      "5.221558570861816\n",
      "5.308377265930176\n",
      "5.10059118270874\n",
      "5.8380327224731445\n",
      "5.567768573760986\n",
      "5.307631492614746\n",
      "5.5744147300720215\n",
      "4.93808650970459\n",
      "5.590235710144043\n",
      "5.3639116287231445\n",
      "5.537886142730713\n",
      "5.429990768432617\n",
      "5.537242412567139\n",
      "5.325555324554443\n",
      "5.143828868865967\n",
      "5.589842319488525\n",
      "5.291086673736572\n",
      "5.6072587966918945\n",
      "5.248784065246582\n",
      "5.357433319091797\n",
      "5.2308349609375\n",
      "5.316965579986572\n",
      "5.107672691345215\n",
      "5.206173896789551\n",
      "5.425546646118164\n",
      "5.44101619720459\n",
      "5.351442337036133\n",
      "5.350681304931641\n",
      "5.533126354217529\n",
      "5.436598777770996\n",
      "5.52046012878418\n",
      "5.364247798919678\n",
      "5.256496429443359\n",
      "5.50817346572876\n",
      "5.485506057739258\n",
      "5.23010778427124\n",
      "5.573274612426758\n",
      "5.622135162353516\n",
      "5.320450782775879\n",
      "5.50822639465332\n",
      "4.973836898803711\n",
      "5.630475997924805\n",
      "5.277328968048096\n",
      "5.182821273803711\n",
      "5.455272197723389\n",
      "4.967054843902588\n",
      "5.430728912353516\n",
      "5.3516845703125\n",
      "5.432618141174316\n",
      "5.54384183883667\n",
      "5.456140041351318\n",
      "5.114681720733643\n",
      "5.4627485275268555\n",
      "5.185734748840332\n",
      "4.7225728034973145\n",
      "5.53387451171875\n",
      "5.0427632331848145\n",
      "5.431127548217773\n",
      "5.518110275268555\n",
      "5.280355453491211\n",
      "5.42125940322876\n",
      "5.10647439956665\n",
      "5.166477203369141\n",
      "5.214332580566406\n",
      "5.135299205780029\n",
      "5.422657012939453\n",
      "5.426476955413818\n",
      "5.382232666015625\n",
      "5.1243720054626465\n",
      "5.4438557624816895\n",
      "5.2391676902771\n",
      "5.184787750244141\n",
      "5.240190505981445\n",
      "5.2438154220581055\n",
      "5.345378398895264\n",
      "5.2884931564331055\n",
      "5.488389492034912\n",
      "5.033004283905029\n",
      "4.970484256744385\n",
      "5.159576416015625\n",
      "5.531126976013184\n",
      "5.090672969818115\n",
      "5.560017108917236\n",
      "5.194899559020996\n",
      "4.94365930557251\n",
      "5.285027980804443\n",
      "5.466447830200195\n",
      "5.253252029418945\n",
      "5.454059600830078\n",
      "5.584757328033447\n",
      "4.954580307006836\n",
      "5.14539098739624\n",
      "5.176708221435547\n",
      "5.157695770263672\n",
      "5.401336669921875\n",
      "5.2595086097717285\n",
      "5.217079162597656\n",
      "5.242090225219727\n",
      "5.432980060577393\n",
      "5.0527753829956055\n",
      "4.995904922485352\n",
      "5.203510284423828\n",
      "5.012720108032227\n",
      "5.485607624053955\n",
      "5.456260681152344\n",
      "5.180938720703125\n",
      "5.217064380645752\n",
      "5.0978474617004395\n",
      "5.210771560668945\n",
      "5.325921058654785\n",
      "5.304800510406494\n",
      "5.471567153930664\n",
      "5.517251014709473\n",
      "5.178229331970215\n",
      "5.336787700653076\n",
      "5.10723876953125\n",
      "5.147545337677002\n",
      "4.991219997406006\n",
      "5.24214506149292\n",
      "5.467710494995117\n",
      "5.505642414093018\n",
      "5.461318492889404\n",
      "5.395580291748047\n",
      "4.73961067199707\n",
      "5.2213568687438965\n",
      "4.954727649688721\n",
      "5.524405479431152\n",
      "5.693783760070801\n",
      "5.333764553070068\n",
      "5.119934558868408\n",
      "5.173250675201416\n",
      "5.178351402282715\n",
      "4.977815628051758\n",
      "5.0382304191589355\n",
      "5.188512802124023\n",
      "4.926041603088379\n",
      "5.128807067871094\n",
      "5.587199687957764\n",
      "5.117501735687256\n",
      "5.182125091552734\n",
      "5.29984188079834\n",
      "5.165740013122559\n",
      "5.187424659729004\n",
      "5.284917831420898\n",
      "5.116123676300049\n",
      "5.059247970581055\n",
      "5.1140570640563965\n",
      "4.944939613342285\n",
      "5.0129618644714355\n",
      "4.906277179718018\n",
      "4.885557174682617\n",
      "5.018137454986572\n",
      "4.788777828216553\n",
      "5.270641326904297\n",
      "5.275097846984863\n",
      "5.065195083618164\n",
      "5.072234153747559\n",
      "5.449095726013184\n",
      "4.920314788818359\n",
      "5.027241230010986\n",
      "5.2593793869018555\n",
      "5.0827717781066895\n",
      "5.363035678863525\n",
      "4.986205101013184\n",
      "5.300133228302002\n",
      "5.388332366943359\n",
      "4.760773181915283\n",
      "5.265203952789307\n",
      "4.996445178985596\n",
      "5.21744441986084\n",
      "4.993747234344482\n",
      "5.014193534851074\n",
      "5.044795036315918\n",
      "5.4764251708984375\n",
      "4.835754871368408\n",
      "4.8210835456848145\n",
      "4.616703510284424\n",
      "5.296675205230713\n",
      "5.285749912261963\n",
      "4.656468868255615\n",
      "4.8554863929748535\n",
      "4.812074661254883\n",
      "4.987018585205078\n",
      "5.39000129699707\n",
      "5.185064792633057\n",
      "4.949300765991211\n",
      "4.989773273468018\n",
      "5.079379558563232\n",
      "5.1467790603637695\n",
      "4.876984596252441\n",
      "5.0655622482299805\n",
      "5.385770320892334\n",
      "5.246435165405273\n",
      "5.0614495277404785\n",
      "4.989788055419922\n",
      "5.1603875160217285\n",
      "5.061074256896973\n",
      "5.215713977813721\n",
      "5.43934440612793\n",
      "5.067005634307861\n",
      "5.290565490722656\n",
      "5.123347759246826\n",
      "5.127200603485107\n",
      "5.375079154968262\n",
      "5.259678840637207\n",
      "4.716119289398193\n",
      "5.235750198364258\n",
      "5.153733730316162\n",
      "5.091805458068848\n",
      "4.978522300720215\n",
      "4.9839959144592285\n",
      "5.007941722869873\n",
      "5.064133167266846\n",
      "5.0896782875061035\n",
      "5.036877632141113\n",
      "4.9719557762146\n",
      "5.201115608215332\n",
      "5.082007884979248\n",
      "5.3959431648254395\n",
      "4.876800537109375\n",
      "5.062695026397705\n",
      "5.217309474945068\n",
      "5.203739166259766\n",
      "4.869172096252441\n",
      "5.139688491821289\n",
      "5.0915045738220215\n",
      "5.23054838180542\n",
      "4.413561820983887\n",
      "4.5116376876831055\n",
      "5.178608417510986\n",
      "4.998919486999512\n",
      "5.216794490814209\n",
      "4.9535112380981445\n",
      "4.783271789550781\n",
      "5.371190071105957\n",
      "5.059171676635742\n",
      "4.958016395568848\n",
      "4.756214618682861\n",
      "5.2512593269348145\n",
      "5.206588268280029\n",
      "4.532852649688721\n",
      "4.82835578918457\n",
      "4.745633602142334\n",
      "4.909582138061523\n",
      "4.962121486663818\n",
      "5.076604843139648\n",
      "4.869241237640381\n",
      "5.0791544914245605\n",
      "5.11820125579834\n",
      "5.305413722991943\n",
      "4.806375980377197\n",
      "5.2411789894104\n",
      "4.701792240142822\n",
      "4.484201431274414\n",
      "5.010015487670898\n",
      "5.347838401794434\n",
      "4.591565132141113\n",
      "5.068691730499268\n",
      "5.34622859954834\n",
      "4.924076557159424\n",
      "4.626847267150879\n",
      "4.823345184326172\n",
      "5.118852138519287\n",
      "4.885679721832275\n",
      "4.601645469665527\n",
      "5.178276062011719\n",
      "4.8377556800842285\n",
      "4.671010971069336\n",
      "4.939436435699463\n",
      "4.851390361785889\n",
      "4.848535060882568\n",
      "4.704967975616455\n",
      "4.630932807922363\n",
      "4.957721710205078\n",
      "4.617198467254639\n",
      "5.042187690734863\n",
      "5.15466833114624\n",
      "5.079761505126953\n",
      "5.0081353187561035\n",
      "4.714084148406982\n",
      "4.9448418617248535\n",
      "4.483203411102295\n",
      "5.077195167541504\n",
      "4.364311695098877\n",
      "5.2139787673950195\n",
      "4.9101691246032715\n",
      "4.450024604797363\n",
      "4.640950679779053\n",
      "4.663082122802734\n",
      "4.920297145843506\n",
      "4.623948574066162\n",
      "5.046638488769531\n",
      "5.007285118103027\n",
      "4.761459827423096\n",
      "4.958741664886475\n",
      "5.280786514282227\n",
      "4.7857794761657715\n",
      "4.659682273864746\n",
      "5.278831958770752\n",
      "4.849565505981445\n",
      "4.765793323516846\n",
      "5.055572986602783\n",
      "4.975930213928223\n",
      "5.033580780029297\n",
      "4.464767932891846\n",
      "4.654800891876221\n",
      "5.026541233062744\n",
      "4.888497352600098\n",
      "4.830872535705566\n",
      "5.1156511306762695\n",
      "4.856328964233398\n",
      "4.727842807769775\n",
      "4.747363090515137\n",
      "4.8673834800720215\n",
      "4.754976749420166\n",
      "4.604757785797119\n",
      "4.671607494354248\n",
      "4.9633917808532715\n",
      "4.807544231414795\n",
      "4.86497688293457\n",
      "4.71113395690918\n",
      "5.060176372528076\n",
      "5.006664276123047\n",
      "4.895511627197266\n",
      "5.035446643829346\n",
      "4.733192443847656\n",
      "5.198714733123779\n",
      "4.865773677825928\n",
      "4.839439868927002\n",
      "4.245296478271484\n",
      "4.826013088226318\n",
      "4.5682268142700195\n",
      "4.920393466949463\n",
      "4.832359313964844\n",
      "4.650084972381592\n",
      "4.905272006988525\n",
      "4.966799259185791\n",
      "4.570574760437012\n",
      "4.950321674346924\n",
      "5.179353713989258\n",
      "4.749907970428467\n",
      "4.976724624633789\n",
      "4.3214006423950195\n",
      "4.769263744354248\n",
      "4.855710506439209\n",
      "5.03171968460083\n",
      "4.717170238494873\n",
      "4.79524040222168\n",
      "5.039768695831299\n",
      "5.074594020843506\n",
      "4.63371467590332\n",
      "4.668942928314209\n",
      "4.663793087005615\n",
      "4.306220054626465\n",
      "4.435708999633789\n",
      "4.64011287689209\n",
      "4.801743984222412\n",
      "4.43235969543457\n",
      "4.80510950088501\n",
      "4.909568786621094\n",
      "4.865536689758301\n",
      "5.20203971862793\n",
      "4.866008281707764\n",
      "4.810690879821777\n",
      "4.609165191650391\n",
      "4.771974086761475\n",
      "4.755284309387207\n",
      "4.853684902191162\n",
      "5.154341220855713\n",
      "4.944756984710693\n",
      "4.330838680267334\n",
      "4.768089771270752\n",
      "4.766275405883789\n",
      "5.005266189575195\n",
      "4.584715843200684\n",
      "4.5700788497924805\n",
      "5.160006999969482\n",
      "5.016785144805908\n",
      "4.750414848327637\n",
      "4.955822467803955\n",
      "5.374257564544678\n",
      "4.852546215057373\n",
      "4.670827865600586\n",
      "4.555818557739258\n",
      "4.706620216369629\n",
      "5.040029048919678\n",
      "5.0146918296813965\n",
      "5.003662109375\n",
      "4.676213264465332\n",
      "4.712147235870361\n",
      "4.743237018585205\n",
      "4.152846813201904\n",
      "4.946806907653809\n",
      "4.9590301513671875\n",
      "4.439501762390137\n",
      "4.9394707679748535\n",
      "4.93391752243042\n",
      "4.4198384284973145\n",
      "5.005603790283203\n",
      "4.593058109283447\n",
      "4.809477806091309\n",
      "4.758476257324219\n",
      "4.9755539894104\n",
      "4.737028121948242\n",
      "4.6313018798828125\n",
      "4.829403877258301\n",
      "4.345179080963135\n",
      "4.827586650848389\n",
      "4.57672643661499\n",
      "4.7501301765441895\n",
      "5.203758239746094\n",
      "5.510741710662842\n",
      "4.581017971038818\n",
      "4.515876770019531\n",
      "4.746021747589111\n",
      "4.886312007904053\n",
      "4.617640972137451\n",
      "4.663182258605957\n",
      "4.941374778747559\n",
      "5.021142959594727\n",
      "4.336986064910889\n",
      "5.124692440032959\n",
      "4.765661716461182\n",
      "4.667706489562988\n",
      "4.583408832550049\n",
      "4.9112653732299805\n",
      "4.411089897155762\n",
      "4.725245952606201\n",
      "4.895345687866211\n",
      "4.702447414398193\n",
      "4.913643836975098\n",
      "4.534100532531738\n",
      "4.205467224121094\n",
      "4.4154438972473145\n",
      "4.601097583770752\n",
      "4.84495735168457\n",
      "4.773440361022949\n",
      "4.017136573791504\n",
      "4.881274700164795\n",
      "4.523776531219482\n",
      "4.8336567878723145\n",
      "4.905384063720703\n",
      "4.957204341888428\n",
      "4.763478755950928\n",
      "4.724503517150879\n",
      "4.254823207855225\n",
      "5.271492958068848\n",
      "4.871653079986572\n",
      "4.687443256378174\n",
      "5.211629867553711\n",
      "4.689531326293945\n",
      "4.370827674865723\n",
      "4.509964942932129\n",
      "4.507085800170898\n",
      "4.711648941040039\n",
      "4.530035972595215\n",
      "4.785367965698242\n",
      "4.544075012207031\n",
      "4.639012813568115\n",
      "4.489922523498535\n",
      "4.728896617889404\n",
      "4.416382312774658\n",
      "4.926259517669678\n",
      "4.72336483001709\n",
      "5.142601013183594\n",
      "4.937084197998047\n",
      "4.7392258644104\n",
      "4.492195129394531\n",
      "4.921753883361816\n",
      "4.876887321472168\n",
      "4.669157028198242\n",
      "4.496392726898193\n",
      "4.263280868530273\n",
      "4.736186504364014\n",
      "4.724349498748779\n",
      "4.536928176879883\n",
      "4.551041603088379\n",
      "4.457595348358154\n",
      "4.807717800140381\n",
      "4.237901210784912\n",
      "4.46675968170166\n",
      "4.294622898101807\n",
      "4.262588024139404\n",
      "4.3433356285095215\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    for inputs,targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs,targets=inputs.to(device),targets.to(device)\n",
    "        loss,logits=model(inputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859914dd-9c57-4640-a699-7e4ad77094cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rotary_embedding_torch\n",
      "  Downloading rotary_embedding_torch-0.8.4-py3-none-any.whl.metadata (678 bytes)\n",
      "Requirement already satisfied: einops>=0.7 in /opt/conda/lib/python3.10/site-packages (from rotary_embedding_torch) (0.8.0)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from rotary_embedding_torch) (2.2.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->rotary_embedding_torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->rotary_embedding_torch) (1.3.0)\n",
      "Downloading rotary_embedding_torch-0.8.4-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: rotary_embedding_torch\n",
      "Successfully installed rotary_embedding_torch-0.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rotary_embedding_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c767b-697e-47ce-b8e6-6fdae0105eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
