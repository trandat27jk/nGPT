{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdfb77f-1d77-4612-8412-98c4a2da06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from torch.nn.utils.parametrize import register_parametrization\n",
    "\n",
    "#parallel computing\n",
    "from torch.distributed import init_process_group,destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "class ModelConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    heads: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    bias: bool = False\n",
    "    parametrize: bool = True\n",
    "    factor: int = 4\n",
    "    dropout: float=0.0\n",
    "\n",
    "\n",
    "class AttentionConfig:\n",
    "    d: int = -1\n",
    "    groups: int = 1\n",
    "    norm_eps: int = 0\n",
    "    eps: float = 1e-6\n",
    "    init_scale = 1\n",
    "    scale: int = 1\n",
    "\n",
    "\n",
    "class FFConfig:\n",
    "    d: int = -1\n",
    "    groups: int = 1\n",
    "    norm_eps: int = 0\n",
    "    eps: float = 1e-6\n",
    "    init_scale: int = 1\n",
    "    scale: int = 1\n",
    "\n",
    "\n",
    "def exist(v):\n",
    "    return v is not None\n",
    "\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exist(v) else d\n",
    "\n",
    "\n",
    "def l2Norm(x, d=-1, groups=1, eps=1e-6, norm_eps=0):\n",
    "    eps = default(eps, 1e-5 if x.dtype == torch.float16 else 1e-10)\n",
    "\n",
    "    if groups > 1:\n",
    "        x = x.chunk(groups, dim=d)\n",
    "        x = torch.stack(x)\n",
    "\n",
    "    if norm_eps == 0:\n",
    "        x_norm = F.normalize(x, dim=d, p=2, eps=eps)\n",
    "\n",
    "    if norm_eps != 0:\n",
    "        norm = x.norm(dim=d, keepdim=True)\n",
    "        d_norm = norm.detach().clamp(min=1 - norm_eps, max=1 + norm_eps)\n",
    "        divisor = norm / d_norm\n",
    "        x_norm = x / divisor.clamp(min=eps)\n",
    "\n",
    "    if groups > 1:\n",
    "        x_norm = torch.cat([*x_norm], dim=d)\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, d=-1, groups=1, eps=1e-6, norm_eps=0):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.groups = groups\n",
    "        self.eps = eps\n",
    "        self.norm_eps = norm_eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return l2Norm(\n",
    "            x, d=self.d, groups=self.groups, eps=self.eps, norm_eps=self.norm_eps\n",
    "        )\n",
    "\n",
    "\n",
    "class LinearNormWeight(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        parametrize=False,\n",
    "        groups=1,\n",
    "        d=-1,\n",
    "        eps=1e-6,\n",
    "        norm_eps=0,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = groups**-1\n",
    "        self.parametrize = parametrize\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "        self.L2Norm = L2Norm(d, groups, eps, norm_eps)\n",
    "        if parametrize:\n",
    "            register_parametrization(self.linear, \"weight\", self.L2Norm)\n",
    "\n",
    "        self.norm_weight_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def norm_weight_(self):\n",
    "        if self.parametrize:\n",
    "            norm = self.weights\n",
    "            original = self.linear.parametrizations.weight.original\n",
    "            original.copy_(norm)\n",
    "        else:\n",
    "            self.weights.copy_(self.L2Norm(self.weights))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.linear.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * self.scale\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, dim, init_scale=1, scale=1):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.ones(dim) * scale)\n",
    "        self.divide_scale = init_scale / scale\n",
    "\n",
    "    def forward(self):\n",
    "        return self.params * self.divide_scale\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, args_attn: AttentionConfig):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.to_q = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "        self.to_k = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "        self.to_v = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "\n",
    "        self.dim_head = args.n_embd // args.heads\n",
    "        self.n_heads = args.heads\n",
    "        self.softmax_scale = self.dim_head**0.5\n",
    "        self.q_scale = Scale(args.n_embd, 1, args.n_embd ** (-0.5))\n",
    "        self.k_scale = Scale(args.n_embd, 1, args.n_embd ** (-0.5))\n",
    "        self.rotary_embed=RotaryEmbedding(self.dim_head)\n",
    "        self.flash=hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        self.dropout=args.dropout\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\n",
    "                \"mask\",\n",
    "                torch.tril(\n",
    "                    torch.ones(args.block_size, args.block_size).view(\n",
    "                        1, 1, args.block_size, args.block_size\n",
    "                    )\n",
    "                ),)\n",
    "            \n",
    "        self.c_proj = LinearNormWeight(\n",
    "            args.n_embd,\n",
    "            args.n_embd,\n",
    "            args.parametrize,\n",
    "            args_attn.groups,\n",
    "            args_attn.d,\n",
    "            args_attn.eps,\n",
    "            args_attn.norm_eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\n",
    "        q = self.rotary_embed.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_embed.rotate_queries_or_keys(k)\n",
    "    \n",
    "        q = q * rearrange(self.q_scale(), \"(h d) -> h 1 d\", h=self.n_heads)\n",
    "        k = k * rearrange(self.q_scale(), \"(h d) -> h 1 d\", h=self.n_heads)\n",
    "        if self.flash:\n",
    "            attn=torch.nn.functional.scaled_dot_product_attention(q,k,v,attn_mask=None,dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1, -2)\n",
    "    \n",
    "            attn = attn * self.softmax_scale\n",
    "    \n",
    "            attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = torch.matmul(attn, v)\n",
    "        out = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.c_proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, args_ffn: FFConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = args.factor * args.n_embd\n",
    "        self.w1 = LinearNormWeight(args.n_embd, hidden_dim)\n",
    "        self.w2 = LinearNormWeight(hidden_dim, args.n_embd)\n",
    "        self.w3 = LinearNormWeight(args.n_embd, hidden_dim)\n",
    "\n",
    "        self.scale_u = Scale(\n",
    "            hidden_dim, init_scale=args_ffn.init_scale, scale=args_ffn.scale\n",
    "        )\n",
    "        self.scale_v = Scale(\n",
    "            hidden_dim, init_scale=args_ffn.init_scale, scale=args_ffn.scale\n",
    "        )\n",
    "        self.scale_ = hidden_dim**0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = self.w1(x)*self.scale_u()\n",
    "        \n",
    "        v = self.w3(x)*self.scale_v()\n",
    "\n",
    "        v = v * self.scale_\n",
    "\n",
    "        return self.w2(F.silu(v) * u)\n",
    "\n",
    "\n",
    "class Lerp_Residual(nn.Module):\n",
    "    def __init__(self, args: ModelConfig, index_layer, fc):\n",
    "        super().__init__()\n",
    "        self.fc = fc\n",
    "        self.l2Norm = L2Norm(d=-1)\n",
    "        self.scale = Scale(\n",
    "            args.n_embd, init_scale=(0.05 / (index_layer+1)), scale=args.n_embd ** (-0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        connect_ = x\n",
    "        out = self.l2Norm(self.fc(x, **kwargs))\n",
    "        out = torch.lerp(connect_, out, self.scale())\n",
    "\n",
    "        return self.l2Norm(out)\n",
    "\n",
    "\n",
    "class nGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, args: ModelConfig, args_attn: AttentionConfig, args_ffn: FFConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_layer = args.n_layer\n",
    "        self.n_attn_layeers = nn.ModuleList(\n",
    "            [Attention(args, args_attn) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.n_ffn_layers = nn.ModuleList(\n",
    "            [FeedForward(args, args_ffn) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.residual_attn = nn.ModuleList(\n",
    "            [\n",
    "                Lerp_Residual(args, i, self.n_attn_layeers[i])\n",
    "                for i in range(args.n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_ffn = nn.ModuleList(\n",
    "            [Lerp_Residual(args, i, self.n_ffn_layers[i]) for i in range(args.n_layer)]\n",
    "        )\n",
    "        self.to_logits = LinearNormWeight(args.n_embd, args.vocab_size)\n",
    "        self.scale_logits=Scale(args.vocab_size,1,args.n_embd**-0.5)\n",
    "        self.to_embedding=nn.Embedding(args.vocab_size,args.n_embd)\n",
    "        self.block_size=args.block_size\n",
    "    def forward(self, x,targets=None):\n",
    "        \n",
    "        x=self.to_embedding(x)\n",
    "        B, T, C = x.size()\n",
    "        for residual_attn, residual_ffn in zip(self.residual_attn, self.residual_ffn):\n",
    "            x = residual_attn(x)\n",
    "            x = residual_ffn(x)\n",
    "        logits = (self.to_logits(x)*self.scale_logits())\n",
    "        if targets is not None:\n",
    "            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index=-1)\n",
    "        else: \n",
    "            loss=None\n",
    "\n",
    "        return loss,logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,idx,max_new_tokens,temperature=1.0,top_k=None):\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond=idx if idx.size(1) <self.block_size else idx[:,-self.block_size:]\n",
    "            _,logits=self(idx_cond)\n",
    "            logits=logits[:,-1,:]/temperature\n",
    "            if top_k is not None:\n",
    "                v,_=torch.topk(logits,min(top_k,logits.size(-1)))\n",
    "                logits[logits<v[:,[-1]]]=-float('Inf')\n",
    "            probs=F.softmax(logits,dim=-1)\n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx,idx_next),dim=1)\n",
    "        return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190c5f84-725d-452d-a3f0-a64663a75580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66ac676-1436-41d2-b585-b9fcbcfbc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f8e5e3-9942-4cff-a30b-5c20be49d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4f34b-8eac-4e29-bfe7-1bb8ac7791de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b1d175-5b5c-492a-961d-b7999bd659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nGPT(ModelConfig,AttentionConfig,FFConfig).to(device)\n",
    "# Load the model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1879a781-378f-4bc3-a8b7-419f0398a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(hasattr(torch.nn.functional, 'scaled_dot_product_attention'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ddc75b-c9b7-4e68-bd7b-11591e07cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 190674432\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde67717-b9ea-4b0e-a83e-c04960be1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3202aa-a2f0-4a49-9616-12ed7d685c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.9.11 tiktoken-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa47b8b-e78e-42de-88c1-f7edc564056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rotary_embedding_torch\n",
      "  Downloading rotary_embedding_torch-0.8.4-py3-none-any.whl.metadata (678 bytes)\n",
      "Requirement already satisfied: einops>=0.7 in /opt/conda/lib/python3.10/site-packages (from rotary_embedding_torch) (0.8.0)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from rotary_embedding_torch) (2.2.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->rotary_embedding_torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->rotary_embedding_torch) (1.3.0)\n",
      "Downloading rotary_embedding_torch-0.8.4-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: rotary_embedding_torch\n",
      "Successfully installed rotary_embedding_torch-0.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rotary_embedding_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f14d224-363b-42bb-9fec-3f017f23de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80aabe71-5965-4394-a3af-87c7ef9d891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=torch.tensor([1.5,4.2,6.5,7.8,9.67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b61990-ee23-49ff-9607-52ac0d49b3a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l2norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43ml2norm\u001b[49m(test_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l2norm' is not defined"
     ]
    }
   ],
   "source": [
    "output=l2norm(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65fab3ff-f64d-4f99-82f3-835084d2a27b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94a9b38-ed60-4166-b254-8269f6c0e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073b976c-e3a5-4ff7-a702-e7a02002f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nGPTDataset(Dataset):\n",
    "    def __init__(self,txt,tokenizer,block_size,stride):\n",
    "        super().__init__()\n",
    "        self.input_ds=[]\n",
    "        self.target_ds=[]\n",
    "        tokens_data=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0,len(tokens_data)-block_size,stride):\n",
    "            inputs=tokens_data[i:i+block_size]\n",
    "            targets=tokens_data[i+1:i+block_size+1]\n",
    "            self.input_ds.append(inputs)\n",
    "            self.target_ds.append(targets)\n",
    "    def __len__(self):\n",
    "        return len(self.input_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ds[idx],self.target_ds[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt,block_size=256,stride=128,batch_size=4,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=nGPTDataset(txt,tokenizer,block_size,stride)\n",
    "    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers,collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddf6ed-e6ae-4d99-b086-0b51b1c619ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1e8678-b8f2-4f62-b243-d6a0b0adb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\",\"r\") as f:\n",
    "    data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27adb22f-73ba-4300-9d81-e88b7741754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=create_dataloader(data,block_size=256,stride=256,batch_size=4,shuffle=True,drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671fba13-ff0e-43ee-ae8a-8dee2c499ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "631e77b0-de2d-4e07-b436-b2b69d5d3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d18882e-7ee0-4304-bdd4-2372b7b1dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34126f69-4879-4364-913e-08e16b8d3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.8282976150512695: time per epoch53.448368310928345\n",
      "loss: 4.626946926116943: time per epoch53.04391622543335\n",
      "loss: 4.311898708343506: time per epoch53.04680037498474\n",
      "loss: 3.9512829780578613: time per epoch53.0349657535553\n",
      "loss: 3.6069490909576416: time per epoch53.11629557609558\n",
      "loss: 3.333874464035034: time per epoch53.05999422073364\n",
      "loss: 3.0872554779052734: time per epoch52.99052286148071\n"
     ]
    }
   ],
   "source": [
    "loss_first=1.7\n",
    "for i in range(1000):\n",
    "    t0=time.time()\n",
    "    optimizer.zero_grad()\n",
    "    for inputs,targets in dataloader:\n",
    "        inputs,targets=inputs.to(device),targets.to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device,dtype=torch.bfloat16):\n",
    "            loss,logits=model(inputs,targets)\n",
    "        loss.backward()\n",
    "        norm=torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step()\n",
    "        if device==\"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1=time.time()\n",
    "        dtime=t1-t0\n",
    "        if loss_first>loss.item():\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "            loss_first=loss.item()\n",
    "    print(f\"loss: {loss.item()}: time per epoch{dtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d9e053b0-9d8f-4849-8a3c-6900ba86229f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1427814074.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[101], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def Generate(model,idx,max_new_tokens,context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=ix if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859914dd-9c57-4640-a699-7e4ad77094cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rotary_embedding_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c197005e-6314-435f-94e8-6cfba329ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=\"hello, how are you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f31c88-2e0d-40cd-8b08-2a3bff13a710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "37d160b1-e138-4787-bb6a-04074ecaec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "encoded=tokenizer.encode(inputs)\n",
    "encoded_tensor=torch.tensor(encoded).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "46121e79-4de6-467e-ad2b-74baf10b1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tensor=encoded_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b45c767b-697e-47ce-b8e6-6fdae0105eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.generate(encoded_tensor,max_new_tokens=10,temperature=1.0,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "af8c62b1-44ef-4ab2-81dd-e46203a321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text=tokenizer.decode(out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "db17479e-4776-480e-8727-3d05faed895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, how are youEdward Hag drumearancesthia freeingBonus Strategy Inquisitioniate\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "06f74d27-67b2-48c8-8777-393685c6c991",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tiktoken/core.py:254\u001b[0m, in \u001b[0;36mEncoding.decode\u001b[0;34m(self, tokens, errors)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Sequence[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'tokens': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    " decoded_text = tokenizer.decode(out.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1c31c-45fd-4660-9690-c8bcc74aa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'out' is a tensor\n",
    "tokens = out.squeeze().tolist()  # Remove unnecessary dimensions and convert to list\n",
    "\n",
    "# Decode tokens to text\n",
    "decoded_text = tokenizer.decode(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
